- Overview
  - Talk on Search Engines + RAG
  - Search Engines
    - Why? - Can't scroll, Used everywhere
    - Why not dump into GPT? - Context Overflow + AI is expensive (time + resources)
  - RAG
    - Why? - Accurate + Enhance + Maybe simplify + Better Understanding
- Search Algorithms
  - Preprocessing
    - Exact match use cases (not semantic)
    - Casing, punctuations
    - Tokenizing, fuzzy, stop words (meaningless - the = the - domain specific)
      - searching "const"
  - Full Text Search
    - We search the entire document for the query
    - We return the documents that contain the query (or a subset of the query)
    - Without ranking, we return the documents in the order they appear in the documents
    - Random token can appear in many documents, so we need to rank them
  - TF-IDF
    - We create indexes to make things fast (caching)
    - TF: Term Frequency - How many times does the term appear in the document?
    - IDF: Inverse Document Frequency - How rare is the term in the set (excluding stop words)?
    - Used to rank documents based on the importance of the terms in the query
    - TF-IDF = TF * (IDF = Number of documents / Number of documents containing the term)
    - TF-IDF is a measure of the importance of a term in all documents
    - The higher the TF-IDF, the more important the term is and vice versa
    - We can use TF-IDF to rank documents based on the importance of the terms in the query
    - We would want the documents having frequent terms from the query but also rare in the set to be ranked higher
  - BM25 TF-IDF
    - The problems with normal IDF
      - Very rare terms will have a very high IDF score
      - Very common terms will have a very low IDF score
      - log((N - Tf + 0.5) / (Tf + 0.5) + 1)
    - The problems with normal TF
      - Linear scaling
      - Very common terms will have a very high TF score (irrelevant keywords)
      - Very rare terms will have a very low TF score (It might be important, but not common)
      - (Tf * (k1 + 1)) / (Tf + k1)
      - Mean vs Median example
      - Document Length Normalization
        - We want to normalize the document length to the average length of the documents
        - 1 - b + b * (dl / avgdl)
  - Semantic Search
    - Problems with keyword search
      - Only exact matches are considered
      - No search with context, synonyms, antonyms, or related words
    - Selecting a model (based on use case, cost, performance, etc.)
      - Multilingual models
      - Domain specific models
    - Dimensions
      - Vector operations
    - Similarity search
      - Dot product (dp = a1 * b1 + a2 * b2 + ... + an * bn = direction similarity and don't consider magnitude/length)
      - Cosine similarity (cos(theta) = dp / (sqrt(a1^2 + a2^2 + ... + an^2) * sqrt(b1^2 + b2^2 + ... + bn^2)) = direction similarity and consider magnitude/length)
      - Similarity depends on the model used
      - Tokenization (static doesn't work as model is not trained on static tokens)
      - Vector DBs - specialized for vector storage and retrieval
      - Hot and cold game on Reddit
    - LSH (Locality Sensitive Hashing)
      - Hashing the vectors to a smaller space
      - May miss some similar vectors
    - Chunking
      - Fixed size chunking
      - Overlapping chunks
      - Semantic chunks
      - Needs lots of debugging and testing to cover edge cases and get the best results
        - Tables, multi page paragraphs, etc.
        - Headers, footers - repeated content across chunks
        - Image captions getting mixed up with text content
        - Column layouts, weird spacing/fonts, missing line breaks
        - Markdown formats
      - ColBERT - created embeddings per token
      - Late chunking - chunking after the embedding is created - summarization, etc. (pronouns, etc.)
      - Try using third party services to see if there's any improvements before implementing your own
  - Hybrid Search
    - Normalizing the scores from the different search algorithms
    - Basic - Weighted combination of the scores
      - Weighted - alpha (0.5 or 0.3 whatever works best for the use case)
      - Combination - balanced - not really well for one but really bad for the other - outer join
      - formula: alpha * bm25Score + (1 - alpha) * semanticScore
    - Reciprocal Rank Fusion
      - Basic hybrid search can penalize one algorithm over the other - hard to normalize the scores
      - Reciprocal Rank Fusion would boost results which are good in both algorithms - only cares about ranks and not the scores
      - formula: 1 / (rank + k)
  - LLM
    - Query Enhancement
      - User query can be noisy, incomplete, or misspelled
      - Spell Correction - correct spelling errors
      - Query rewriting - semantic meanings to keywords using model knowledge
      - Query expansion - related terms/keywords, synonyms, antonyms, etc.
    - Re-ranking
      - Narrowed results from hybrid search -> re-rank results based on user query for best results
      - LLM based - risk of hallucination
      - Cross-encoder - more accurate than the LLM based
  - Evaluation
    - Manual evaluation
      - Difficult to automate tests due to many factors (query, docs quality, number of results), check what user wants
        - No fixed criteria
        - LLMs can hallucinate
    - Golden dataset
      - Real queries from users
      - Quality documents - verified by domain experts
      - Data annotation
      - Evaluation metrics
    - Precision metrics
      - Like test cases
      - Precision = relevant_retrieved / total_retrieved
        - Of what we retrieved, how much is relevant?
        - Of the things in your net, how many are fish?
      - Recall = relevant_retrieved / total_relevant
        - Of all correct relevant results, how many did we find?
        - Of all fish in the lake, how many did you catch?
      - F1 Score = 2 * (precision * recall) / (precision + recall)
       - Balanced between precision and recall
       - Penalizes imbalances
       - Useful when precision and recall are both important
       - Ranking is ignored
    - Error analysis
      - These metrics are like stack traces
      - Instead of tweaking the parameters or retrieving more results, we can try to understand the data and the queries to improve the retrieval of results
      - We need to debug each step of the pipeline to understand the problems
      - Test different queries and see how the results change
    - LLM Evaluation
      - Engineer prompt according to the use case (by experts)
  - Augmented Generation
    - Q&A
    - Summary
    - Citation
  - Agentic RAG
    - Recursive retrieval
    - LLM controlled
    - Tool calls in a loop
    - Slower and expensive
  - Multi-modal RAG
    - Text, images, videos, audio, etc.
    - With multi-modal LLM
    - Comparable
      - Convert image to text using LLM to make them comparable
      - Use image alt text (to minimize the distance between the image and the text embeddings)
      - Text surrounded by image
